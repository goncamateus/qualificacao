
% resumo em português
\begin{resumo}[Resumo] 
Criar um ambiente de treinamento para Aprendizado por Reforço (Reinforcement Learning -- RL) é uma dificuldade conhecida na área. \textbf{Quando a recompensa é composta por diferentes sinais}, a definição dos pesos de cada sinal representa o currículo de \textbf{aprendizado} que o agente seguirá durante o treinamento. O processo de testar novos pesos perdura até que o agente atinja o objetivo do ambiente. Apresentamos o DyLam, um \textbf{framework adaptativo de aprendizado por currículo para algoritmos de RL}. \textbf{Ao exigir apenas uma estimativa} do máximo e do mínimo teóricos de cada \textbf{componente de recompensa}, o DyLam é capaz de \textbf{ajustar} seus pesos de forma \textbf{dinâmica} durante o treinamento, dependendo de qual sinal precisa ser otimizado em cada estágio do aprendizado. Propomos matematicamente uma extensão tanto para frameworks de RL baseados em Valor quanto em Gradiente de Política e \textbf{demonstramos experimentalmente a robustez} desse método em comparação com \textbf{o estado da arte em diferentes ambientes de benchmark de RL}.
% \noindent %- o resumo deve ter apenas 1 parágrafo e sem recuo de texto na primeira linha, essa tag remove o recuo. Não pode haver quebra de linha.

 \vspace{\onelineskip}
    
 \noindent
 \textbf{Palavras-chaves}: Aprendizagem por Reforço. Curriculum Learning. Otimização multi-objetivo.
\end{resumo}



% resumo em inglês
\begin{resumo}[Abstract]
\begin{otherlanguage*}{english}

 %\noindent
Creating a Reinforcement Learning (RL) training environment is a known difficulty in the field. \textbf{When the reward is a composition of different signals}, defining the weights for each signal represents the \textbf{learning} curricula the agent will follow during training. The process of trying new weights endures until the agent reaches the objective of the environment. We present DyLam, a robust \textbf{adaptative curriculum learning framework for RL algorithms}. \textbf{By requiring only an estimate} of the theoretical maximum and minimum of each \textbf{reward component}, DyLam can \textbf{adjust} its weights \textbf{dynamically} during training, depending on which signal needs to be optimized at each training stage. We mathematically propose an extension of both Value-based and Policy Gradient RL frameworks, and \textbf{show experimentally the robustness} of this method compared to \textbf{the state-of-the-art in different RL benchmark environments}.



   \vspace{\onelineskip} 
 
   \noindent 
   \textbf{Keywords}: Reinforcement Learning. Curriculum Learning. Multi-Objective Optimization.
 \end{otherlanguage*}
 \end{resumo}
