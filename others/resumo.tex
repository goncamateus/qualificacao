
% resumo em português
\begin{resumo}[Resumo] 
Criar um ambiente de treinamento para Aprendizado por Reforço (RL) é uma dificuldade conhecida na área. \textbf{Quando a recompensa é uma composição de diferentes sinais}, definir os pesos para cada sinal representa o currículo de \textbf{aprendizado} que o agente seguirá durante o treinamento. O processo de experimentação com novos pesos se prolonga até que o agente atinja o objetivo do ambiente. Apresentamos DyLam, uma estrutura robusta de \textbf{auto-currículo de aprendizado automatizado para algoritmos de RL}. \textbf{Exigindo apenas uma estimativa} do máximo e mínimo teórico de cada \textbf{componente da recompensa}, DyLam pode \textbf{ajustar} seus pesos \textbf{dinamicamente} durante o treinamento, dependendo de qual sinal precisa ser otimizado em cada estágio do treinamento. \textbf{Demonstramos experimentalmente a robustez} desse método em comparação com \textbf{o estado da arte no contexto do \textit{benchmark} de controle discreto \textit{Lunar Lander}}.
% \noindent %- o resumo deve ter apenas 1 parágrafo e sem recuo de texto na primeira linha, essa tag remove o recuo. Não pode haver quebra de linha.

 \vspace{\onelineskip}
    
 \noindent
 \textbf{Palavras-chaves}: Aprendizagem por Reforço. Curriculum Learning. Otimização multi-objetivo.
\end{resumo}



% resumo em inglês
\begin{resumo}[Abstract]
\begin{otherlanguage*}{english}

 %\noindent
Creating a Reinforcement Learning (RL) training environment is a known difficulty in the field. \textbf{When the reward is a composition of different signals}, defining the weights for each signal represents the \textbf{learning} curricula the agent will follow during training. The process of trying new weights endures until the agent reaches the objective of the environment. We present DyLam, a robust automated \textbf{self-curriculum learning framework for RL algorithms}. \textbf{By requiring only an estimate} of the theoretical maximum and minimum of each \textbf{reward component}, DyLam can \textbf{adjust} its weights \textbf{dynamically} during training, depending on which signal needs to be optimized at each training stage. We \textbf{show experimentally the robustness} of this method compared to \textbf{the state-of-the-art in the Lunar Lander discrete control benchmark context}.



   \vspace{\onelineskip} 
 
   \noindent 
   \textbf{Keywords}: Reinforcement Learning. Curriculum Learning. Multi-Objective Optimization.
 \end{otherlanguage*}
 \end{resumo}
