\subsection{Soft Actor-Critic}
\label{sec:sac}

The \gls{sac} algorithm extends the deterministic actor-critic approach by addressing some of the limitations of \gls{ddpg}, such as poor exploration and training instability \cite{sac}. While \gls{ddpg} uses a deterministic policy and requires carefully tuned exploration noise, \gls{sac} employs a \emph{stochastic policy} and introduces the concept of \emph{entropy maximization} as part of the optimization objective.

The key idea behind \gls{sac} is to encourage exploration by maximizing a trade-off between expected return and the entropy of the policy. Formally, the objective becomes:

\begin{equation}
\label{eq:sac_objective}
J(\pi) = \sum_{t=0}^{T} \E_{(s_t,a_t)\sim \rho_\pi} \left[ r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \right],
\end{equation}

\noindent
where $\mathcal{H}(\pi(\cdot|s_t)) = \E_{a_t \sim \pi}[-\log \pi(a_t|s_t)]$ is the entropy of the policy at state $s_t$, and $\alpha$ is a temperature parameter that controls the relative importance of the entropy term.

By encouraging higher entropy (i.e., more randomness), SAC ensures better exploration and robustness during training. Unlike \gls{ddpg}, \gls{sac} can learn from a wider range of trajectories without collapsing to suboptimal deterministic behaviors.

\gls{sac} introduces several innovations over \gls{ddpg}:

\begin{itemize}
    \item \textbf{Stochastic Policies:} Instead of outputting a single deterministic action, SAC learns a policy $\pi(a|s)$ that outputs a distribution over actions, typically modeled by a squashed Gaussian.
    
    \item \textbf{Entropy-Regularized Objective:} As seen in Equation~\ref{eq:sac_objective}, SAC maximizes both expected reward and entropy, making the agent more exploratory and less likely to get stuck in poor local optima.
    
    \item \textbf{Twin Q-networks:} \gls{sac} uses two critic networks to mitigate overestimation bias. The minimum of the two Q-values is used in the target computation.
    
    \item \textbf{Automatic Temperature Adjustment:} \gls{sac} can learn the temperature parameter $\alpha$ automatically by adjusting it to match a target entropy level. This removes the need for manual tuning of exploration-exploitation trade-offs.
\end{itemize}

These enhancements make \gls{sac} highly sample-efficient and robust in continuous control tasks. It has shown strong empirical performance across a variety of benchmark environments, often outperforming \gls{ddpg} in both sample efficiency and final performance.

\begin{algorithm}
    \caption{Soft Actor-Critic (SAC) Algorithm}
    \begin{algorithmic}
        \State Initialize actor parameters $\theta$, critic parameters $w_1$, $w_2$, temperature parameter $\alpha$
        \State Initialize target critic parameters $w_1' \gets w_1$, $w_2' \gets w_2$
        \State Initialize empty replay buffer $\mathcal{D}$
        \For{episode = 1 to M}
            \State Receive initial state $s_0$
            \For{$t = 1$ to T}
                \State Sample action $a_t \sim \pi_\theta(\cdot|s_t)$
                \State Execute action $a_t$, observe reward $r_t$ and next state $s_{t+1}$
                \State Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$
                
                \State Sample minibatch $\{(s_i, a_i, r_i, s_{i+1})\}_{i=1}^N$ from $\mathcal{D}$
                \State Sample $a_{i+1} \sim \pi_\theta(\cdot|s_{i+1})$ and compute $\log \pi_\theta(a_{i+1}|s_{i+1})$
                \State $y_i \gets r_i + \gamma \left( \min_{j=1,2} Q_{w_j'}(s_{i+1}, a_{i+1}) - \alpha \log \pi_\theta(a_{i+1}|s_{i+1}) \right)$
                
                \State Update critics by minimizing:
                \[
                L(w_j) = \frac{1}{N} \sum_i (Q_{w_j}(s_i, a_i) - y_i)^2, \quad \text{for } j = 1, 2
                \]
                
                \State Update actor by minimizing:
                \[
                J(\theta) = \frac{1}{N} \sum_i \left( \alpha \log \pi_\theta(a_i|s_i) - \min_{j=1,2} Q_{w_j}(s_i, a_i) \right)
                \]

                \State Update temperature (optional, if $\alpha$ is learned):
                \[
                J(\alpha) = \frac{1}{N} \sum_i \alpha \left( -\log \pi_\theta(a_i|s_i) - \mathcal{H}_{\text{target}} \right)
                \]

                \State Soft update target networks:
                \[
                w_j' \gets \tau w_j + (1 - \tau) w_j', \quad \text{for } j = 1, 2
                \]
            \EndFor
        \EndFor
    \end{algorithmic}
    \label{algo:sac}
\end{algorithm}

The methods reviewed in this chapter differ in their learning dynamics but share a fundamental reliance on the reward signal as the central driver of optimization. In this sense, reward design and prioritization constitute a common challenge across all frameworks. DyLam is designed as an algorithm-agnostic mechanism and can be combined with each of these methods without modifying their underlying update rules. By decoupling reward prioritization from the learning algorithm, DyLam provides an unified and principled way to handle decomposed reward structures across different Reinforcement Learning paradigms.