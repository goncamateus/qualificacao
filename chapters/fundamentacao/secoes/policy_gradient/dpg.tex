\subsection{Deterministic Policy Gradient Methods}
\label{sec:dpg}

The \gls{dpg} algorithm was introduced as an \textbf{off-policy} Actor–Critic method capable of learning a deterministic target policy from data generated by a separate behavior policy \cite{dpg}. This formulation builds upon earlier off-policy policy gradient approaches, such as \cite{offpac}, in which a behavior policy $\beta(a \mid s)$—distinct from the target policy $\pi_{\theta}(a \mid s)$—is used to collect experience for policy updates.

In contrast to stochastic policy gradient methods, which learn a probability distribution over actions $\pi_\theta(a \mid s)$, \gls{dpg} maintains a parameterized deterministic policy $\mu(s \mid \theta_\mu)$ that directly maps each state to a single action. That is, instead of estimating action probabilities, the actor explicitly estimates the action to be executed in a given state, which motivates the term deterministic. Exploration is therefore handled externally, typically by injecting noise into the behavior policy used for data collection, rather than being encoded in the policy representation itself~\cite{ddpg}.

The critic, denoted $Q_w(s,a)$, approximates the action–value function and is trained using the Bellman equation (Equation~\ref{eq:bellman_value}). The learning objective of the actor is to maximize the expected return induced by the deterministic policy,
\begin{equation}
\label{eq:dpg_objective}
J(\theta_\mu) = \E_{s_t \sim p^\beta}\left[ Q_w\big(s_t, \mu(s_t \mid \theta_\mu)\big) \right],
\end{equation}
where $p^\beta$ denotes the state distribution induced by the behavior policy $\beta$.

Applying the deterministic policy gradient theorem, the gradient of this objective with respect to the actor parameters can be written as
\begin{gather}
\begin{split}
\label{eq:dpg}
\nabla_{\theta_\mu} J(\theta_\mu)
&\approx \E_{s_t \sim p^\beta}\left[\nabla_{\theta_\mu} Q_w(s,a)\big|{s=s_t,, a=\mu(s_t)}\right] \\
&= \E{s_t \sim p^\beta}\left[\nabla_{a} Q_w(s,a)\big|{s=s_t,, a=\mu(s_t)}
\cdot \nabla{\theta_\mu} \mu(s \mid \theta_\mu)\big|_{s=s_t}\right].
\end{split}
\end{gather}

While \gls{dpg} is effective in continuous action spaces, its performance is often limited by the function approximator’s capacity to estimate $Q$ accurately—especially in high-dimensional state spaces. To address this, \cite{ddpg} proposed the \gls{ddpg} algorithm, inspired by the \gls{dqn} architecture. \gls{ddpg} incorporates deep neural networks to approximate both the actor and the critic, and leverages Experience Replay for sample efficiency and stability.

In addition to these features, \gls{ddpg} introduces several techniques to stabilize training:

- \textbf{Target Networks:} As in \gls{dqn} \cite{dqnTarget}, separate target networks are used for both the actor and critic to prevent harmful feedback loops. These target networks are updated slowly using a soft update rule: 
  \[
  \theta' \gets \tau\theta + (1 - \tau)\theta', \quad 0 < \tau \ll 1,
  \]
  where $\theta'$ are the target weights, and $\theta$ are the main network’s weights.

- \textbf{Exploration Noise:} Since \gls{ddpg} learns a deterministic policy, it requires an external mechanism for exploration. The algorithm employs an Ornstein-Uhlenbeck process \cite{uhlenbeck1930theory} to generate temporally correlated noise, which is added to the actions during training. 