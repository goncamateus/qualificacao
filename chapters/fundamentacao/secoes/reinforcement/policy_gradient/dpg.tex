\subsubsection{Deterministic Policy Gradient Methods}
\label{sec:dpg}

The \gls{dpg} algorithm was introduced as an \textbf{off-policy} Actor-Critic method capable of learning a deterministic target policy from data generated by a separate behavior policy \cite{dpg}. This approach extends the ideas from \cite{offpac}, in which a behavior policy $\beta(a|s)$, different from the target policy $\pi_{\theta}(a|s)$, is used to generate trajectories for policy updates.

\gls{dpg} maintains a parameterized deterministic policy, denoted $\mu(s|\theta_\mu)$, which maps each state to a specific action. The critic, $Q_w(s,a)$, estimates the action-value function and is updated using the Bellman equation (Equation~\ref{eq:bellman_value}). The actor is trained to maximize the expected return by applying the deterministic policy gradient:

\begin{gather}
\begin{split}
\label{eq:dpg}
\nabla_{\theta_\mu}J &\approx \E_{s_t \sim p^\beta}\left[\nabla_{\theta_\mu}Q(s,a|\theta_Q)\big|_{s=s_t, a=\mu(s_t)}\right] \\
&= \E_{s_t \sim p^\beta}\left[\nabla_{a}Q(s,a|\theta_Q)\big|_{s=s_t, a=\mu(s_t)} \cdot \nabla_{\theta_\mu}\mu(s|\theta_\mu)\big|_{s=s_t}\right],
\end{split}
\end{gather}

\noindent
where $p^\beta$ denotes the state distribution under the behavior policy $\beta$.

While \gls{dpg} is effective in continuous action spaces, its performance is often limited by the function approximator’s capacity to estimate $Q$ accurately—especially in high-dimensional state spaces. To address this, \cite{ddpg} proposed the \gls{ddpg} algorithm, inspired by the \gls{dqn} architecture. \gls{ddpg} incorporates deep neural networks to approximate both the actor and the critic, and leverages Experience Replay for sample efficiency and stability.

In addition to these features, \gls{ddpg} introduces several techniques to stabilize training:

- \textbf{Target Networks:} As in \gls{dqn} \cite{dqnTarget}, separate target networks are used for both the actor and critic to prevent harmful feedback loops. These target networks are updated slowly using a soft update rule: 
  \[
  \theta' \gets \tau\theta + (1 - \tau)\theta', \quad 0 < \tau \ll 1,
  \]
  where $\theta'$ are the target weights, and $\theta$ are the main network’s weights.

- \textbf{Exploration Noise:} Since \gls{ddpg} learns a deterministic policy, it requires an external mechanism for exploration. The algorithm employs an Ornstein-Uhlenbeck process \cite{uhlenbeck1930theory} to generate temporally correlated noise, which is added to the actions during training. 