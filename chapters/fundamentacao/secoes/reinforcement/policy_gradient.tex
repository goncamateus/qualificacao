\subsection{Policy Gradient Methods}
\label{sec:pg}
\gls{pg} methods employ a parameterized policy to select actions directly, without relying on an action-value function during execution. We denote the policy parameters as $\theta \in \R^n$. These parameters are updated throughout training to maximize the expected cumulative reward. Although value functions can be used to guide the update of $\theta$, they are typically not consulted for action selection. The probability of choosing action $a$ in state $s$ at time $t$ under the parameter vector $\theta$ is denoted as:
\[
\pi(a|s, \theta) \doteq Pr\{A_t = a \mid S_t = s, \theta_t = \theta\}.
\]

\gls{pg} methods optimize the policy by estimating the gradient of a performance measure $J(\theta)$ with respect to the policy parameters. The objective is to perform gradient ascent in $J$, following the update rule:

\begin{equation}
\label{eq:gradient_ascent}
    \theta_{t+1} \doteq \theta + \lambda\widehat{\nabla J(\theta_t)},
\end{equation}

\noindent
where $\widehat{\nabla J(\theta_t)}$ is a stochastic estimate of the true gradient. Any method that follows this general framework is considered a policy gradient method. The core theoretical result guiding these updates is summarized in Theorem~\ref{theo:pg_theorem}.

\begin{theorem}[Policy Gradient Theorem]
\label{theo:pg_theorem}
    \begin{gather}
    \begin{split}
    \nabla_\theta J (\pi_\theta) &= \int_S p^\pi(s) \int_A \nabla_\theta \pi_\theta (a|s) Q_\pi (s,a) \, da \, ds \\
    &= \E_{S\sim p^\pi,a\sim \pi_\theta}[\nabla_\theta \log\pi_\theta (a|s) Q_\pi (s,a)]
    \end{split}
    \label{eq:pg_theorem}
    \end{gather} 
\end{theorem}


\input{chapters/fundamentacao/secoes/reinforcement/policy_gradient/ac}
\input{chapters/fundamentacao/secoes/reinforcement/policy_gradient/dpg}
\input{chapters/fundamentacao/secoes/reinforcement/policy_gradient/sac}