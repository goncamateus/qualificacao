\subsection{Bellman Equations}
\label{sec:bellman}

The formal structure of a \gls{mdp} enables us to reason about the quality of states and actions through the concept of value functions. These functions quantify how good it is for an agent to be in a certain state, or to take a particular action in that state, under a given policy. The recursive formulation that defines these value functions is known as the \emph{Bellman equation}~\cite{sutton2018reinforcement}.

Named after Richard Bellman, who introduced dynamic programming in the 1950s \cite{bellman1966dynamic}, the Bellman equations lie at the heart of reinforcement learning. They provide a fundamental insight: the value of a state can be decomposed into the immediate reward and the value of subsequent states. This recursive structure allows learning algorithms to estimate long-term returns without explicitly exploring all future trajectories.

Given a policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ that maps states to actions, the \emph{\textbf{state-value function}}, $v_\pi(s)$ is defined as the expected return when starting in state $s$ and following $\pi$ thereafter:

\begin{equation}
v_\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_{t+1} \,\middle|\, s_0 = s \right].
\end{equation}

This function satisfies the \emph{\textbf{Bellman expectation equation}}:

\begin{equation}
v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s,a) \left[ \mathcal{R}(s,a) + \gamma v_\pi(s') \right].
\label{eq:bellman_value}
\end{equation}

Similarly, the \emph{\textbf{action-value function}} $q_\pi(s,a)$ defines the expected return after taking action $a$ in state $s$ and then following policy $\pi$:

\begin{equation}
q_\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_{t+1} \,\middle|\, s_0 = s, a_0 = a \right],
\end{equation}

\noindent
and satisfies the corresponding Bellman equation:

\begin{equation}
q_\pi(s,a) = \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s,a) \left[ \mathcal{R}(s,a) + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') q_\pi(s',a') \right].
\label{eq:bellman_action_value}
\end{equation}

While the Bellman expectation equations describe the value of a policy $\pi$, the goal in reinforcement learning is often to find an \emph{optimal policy} $\pi^*$â€”one that maximizes the expected return from every state. To reason about this, we define the \emph{optimal value functions}:

\begin{itemize}
    \item The \textbf{optimal state-value function}:
    \begin{equation}
        v_*(s) = \max_\pi v_\pi(s),
    \end{equation}
    which gives the highest expected return achievable from state $s$ under any policy.
    
    \item The \textbf{optimal action-value function}:
    \begin{equation}
        q_*(s,a) = \max_\pi q_\pi(s,a),
    \end{equation}
    which gives the highest expected return achievable from state $s$ when taking action $a$ and following the best policy thereafter.
\end{itemize}

These functions satisfy the \emph{Bellman optimality equations}, which are nonlinear due to the presence of the max operator. The optimal state-value function obeys:

\begin{equation}
v_*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s,a) \left[ \mathcal{R}(s,a) + \gamma v_*(s') \right].
\label{eq:bellman_opt_value}
\end{equation}

Likewise, the optimal action-value function satisfies:

\begin{equation}
q_*(s,a) = \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s,a) \left[ \mathcal{R}(s,a) + \gamma \max_{a' \in \mathcal{A}} q_*(s',a') \right].
\label{eq:bellman_opt_q}
\end{equation}

Solving these equations directly yields the optimal value functions, from which an optimal policy $\pi^*$ can be derived by acting greedily:

\begin{equation}
\pi^*(s) = \arg\max_{a \in \mathcal{A}} q_*(s,a).
\end{equation}

These equations form the theoretical foundation for many dynamic programming and reinforcement learning algorithms, which aim to approximate or iteratively compute $v_*$ or $q_*$ in order to find optimal behavior
