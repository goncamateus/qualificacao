\subsection{Finite Markov Decision Process}
\label{sec:mdp}

The \gls{mdp} is a mathematical framework used to formalize sequential decision-making problems in \gls{rl}, where the agent learns from incremental feedback. It provides a well-defined structure to model the interaction between an agent and its environment. As previously introduced, the agent observes the environment, takes actions, and receives feedback in the form of rewards. This interactive process, at its core, defines a \gls{mdp}~\cite{sutton2018reinforcement}.

\begin{figure}
    \caption{Representation of a \gls{mdp}. Source: Author}
    \centering
    \includegraphics[width=0.75\linewidth]{images/fundamentacao/rl/mdp.png}
    \label{fig:fundamentacao/mdp_basic}
    \par\medskip\ABNTEXfontereduzida\selectfont\textbf{Source:} Author
\end{figure}

Formally, a finite Markov Decision Process is defined by a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where:

\begin{itemize}
    \item $\mathcal{S}$ is a finite set of states.
    \item $\mathcal{A}$ is a finite set of actions.
    \item $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ is the state transition probability function, where $\mathcal{P}(s'|s,a)$ denotes the probability of transitioning to state $s'$ after taking action $a$ in state $s$.
    \item $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, where $\mathcal{R}(s,a)$ denotes the expected immediate reward received after taking action $a$ in state $s$.
    \item $\gamma \in [0,1]$ is the discount factor, which determines the importance of future rewards.
\end{itemize}

At each discrete time step $t = 0, 1, 2, \dots$, the agent observes the current state $s_t \in \mathcal{S}$, selects an action $a_t \in \mathcal{A}(s_t)$, receives a scalar reward $r_{t+1} \in \mathbb{R}$, and transitions to a new state $s_{t+1} \in \mathcal{S}$ according to the transition probability $\mathcal{P}(s_{t+1} | s_t, a_t)$. This process is illustrated in Figure~\ref{fig:fundamentacao/mdp_basic}.



