\subsection{Farama Gymnasium}
\label{sec:gym}

The \textbf{Farama Gymnasium} is a standardized Python library for developing and interacting with reinforcement learning environments~\cite{towers2024gymnasium}. It is the spiritual successor to OpenAI's Gym, which has been a cornerstone for the RL community since its release. With the deprecation of the original Gym repository, the Farama Foundation took the responsibility of maintaining and extending its functionality under the name Gymnasium\footnote{\url{https://github.com/Farama-Foundation/Gymnasium}}. This pattern has become ubiquitous in RL workflows, enabling efficient testing, benchmarking, and integration across tools~\cite{mnih2015human, dqnTarget, ddpg, sac}.

The core idea behind Gymnasium is to provide a unified API for RL environments, enabling researchers and developers to experiment with a wide range of tasks through a common interface. This standardization facilitates reproducibility, benchmarking, and interoperability between agents, environments, and libraries.

\subsubsection{The Environment Interface Pattern}
Gymnasium enforces a clear abstraction separating the \emph{environment} from the \emph{simulator}. The environment handles the agent-environment loop, while the simulator (e.g., physics engines or emulators) encapsulates the dynamics and rendering. This abstraction allows a consistent API regardless of the domain—be it gridworlds~\cite{MinigridMiniworld23}, robotics~\cite{todorov2012mujoco}, or Atari games~\cite{bellemare13arcade}.

To define a new environment in Gymnasium, several components must be specified:

\begin{itemize}
    \item \textbf{Observation space:} Defines the structure and type of states returned to the agent. This could be discrete labels, continuous vectors, images, etc.
    \item \textbf{Action space:} Specifies the valid actions an agent can take. Gymnasium supports discrete, continuous, and compound spaces.
    \item \textbf{Reward signal:} A scalar or vector indicating the immediate feedback for the agent’s action. The semantics are domain-specific but must follow the Gymnasium interface.
    \item \textbf{Core methods:}
    \begin{itemize}
        \item \texttt{reset()}: Initializes the environment to a starting state, returning the initial observation.
        \item \texttt{step(action)}: Advances the environment one timestep given the action. It returns a tuple of $(\text{observation}, \text{reward}, \text{terminated}, \text{truncated}, \text{info})$.
    \end{itemize}
\end{itemize}

\subsubsection{Registration and Wrappers}
Gymnasium uses a registry system to manage available environments, allowing users to instantiate them using a string identifier. Additionally, it provides \emph{wrappers}, which are modular tools for modifying environments (e.g., observation normalization, reward shaping, or frame skipping) without changing their core logic.

\subsubsection{Basic Usage Example}
The following Python snippet demonstrates the basic usage of a Gymnasium environment using the \texttt{LunarLander-v3} domain. It illustrates environment creation, stepping through the simulation, and resetting after episode termination:

\begin{lstlisting}[language=Python, caption=Basic usage of Gymnasium, label=code:gym]
import gymnasium as gym

# Initialise the environment
env = gym.make("LunarLander-v3", render_mode="human")

# Reset the environment to generate the first observation
observation, info = env.reset(seed=42)
for _ in range(1000):
    # This is where you would insert your policy
    action = env.action_space.sample()

    # Step through the environment with the action
    observation, reward, terminated, truncated, info = env.step(action)

    # If the episode has ended, reset to start a new episode
    if terminated or truncated:
        observation, info = env.reset()

env.close()
\end{lstlisting}
