\subsection{Value-Based Methods}
\label{sec:value_based}

Value-based methods in \gls{rl} are a class of algorithms that estimate how good it is to take a certain action in a given state. Instead of learning policies directly, these methods derive them indirectly by estimating value functions and acting greedily with respect to those estimates.

The most fundamental value function in this context is the \textbf{action-value function}, or $Q$-function, which is defined for a policy $\pi$ as:
\begin{equation}
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \,\bigg|\, s_t = s, a_t = a \right],
\end{equation}

\noindent representing the expected return when starting from state $s$, taking action $a$, and thereafter following policy $\pi$. The optimal action-value function $Q^*(s, a)$ maximizes this expectation over all possible policies.

\subsubsection{Q-Learning}
Q-Learning~\cite{watkins1989learning} is an \emph{off-policy} TD control algorithm that estimates $Q^*(s, a)$ directly. It updates its estimates using the Bellman optimality equation:

\begin{equation}
    \label{eq:q_learning_update}
    Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right],
\end{equation}

\noindent where $\alpha$ is the learning rate, $\gamma$ is the discount factor, and the term inside the brackets is known as the \emph{TD target}. Q-Learning builds an estimate of $Q^*$ regardless of the policy being followed during training, making it suitable for off-policy learning. The resulting greedy policy can be extracted as:

\begin{equation}
    \pi^*(s) = \arg\max_a Q^*(s, a).
\end{equation}

To ensure sufficient exploration during learning, an $\varepsilon$-greedy strategy is typically used: the agent selects a random action with probability $\varepsilon$, and the greedy action otherwise.

Algorithm~\ref{alg:q_learning} shows the basic Q-Learning algorithm for finite state and action spaces.

\begin{algorithm}
\caption{Tabular Q-Learning}
\label{alg:q_learning}
\begin{algorithmic}
\Require Step size $\alpha \in (0,1]$, exploration rate $\varepsilon$, discount factor $\gamma$
\State Initialize $Q(s, a)$ arbitrarily for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$
\For {each episode}
    \State Initialize $\mathcal{S}$
    \For {each time step $t$ of the episode}
        \State With probability $\varepsilon$, select $a_t$ randomly
        \State Otherwise, $a_t \gets \arg\max_a Q(s_t, \mathcal{A})$
        \State Take action $a_t$, observe $r_{t+1}, s_{t+1}$
        \State $Q(s_t, a_t) \gets Q(s_t, a) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]$
        \State $s_t \gets s_{t+1}$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Although Q-Learning is guaranteed to converge under certain conditions in tabular settings, it becomes infeasible for large or continuous state spaces due to the curse of dimensionality. This limitation motivated the development of function approximation methods, most notably the \gls{dqn}~\cite{mnih2015human}.

\subsubsection{Deep Q-Networks}
\label{sec:dqn}

As traditional Q-Learning relies on tabular representations, it becomes impractical in environments with large or continuous state spaces. To address this limitation, \cite{mnih2015human} introduced the \gls{dqn}, a framework that combines Q-learning with deep neural networks to approximate the action-value function.

The \gls{dqn} builds upon earlier work such as TD-Gammon~\cite{tesauro1994td}, where a multilayer perceptron (MLP) with a single hidden layer was used to approximate the value function in the TD($\lambda$) framework~\cite{sutton2018reinforcement}. However, DQN represents a major breakthrough by successfully integrating deep learning into reinforcement learning through three key innovations:

\begin{itemize}
    \item \textbf{Convolutional State Representation:} When the agent receives high-dimensional sensory inputs like images, DQN uses Convolutional Neural Networks (CNNs)~\cite{goodfellow2013multi} to extract compact, informative state representations directly from raw pixels.
    
    \item \textbf{Experience Replay:} DQN introduces a memory buffer, often referred to as a \gls{rb}, that stores past transitions $(s, a, r, s')$. During training, mini-batches of these experiences are sampled uniformly at random to update the network. This breaks temporal correlations between samples and improves sample efficiency.
    
    \item \textbf{Target Networks:} To address instability and divergence issues during training, DQN employs a separate target network. This network is a delayed copy of the current policy network and is used to compute the TD target during Q-learning updates. By holding the target values fixed for a number of steps, DQN mitigates the problem of moving targets and reduces the overestimation bias in Q-values~\cite{dqnTarget}.
\end{itemize}

The update rule for DQN is a modified version of the Q-learning update:

\begin{equation}
    \label{eq:dqn_update}
    Q(s_t, a_t; \theta) \leftarrow Q(s_t, a_t; \theta) + \alpha \big[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-) - Q(s_t, a_t; \theta) \big],
\end{equation}

\noindent where $\theta$ represents the parameters of the policy network and $\theta^-$ those of the target network. The parameters $\theta^-$ are periodically updated to match $\theta$ according to a fixed synchronization interval.

These innovations allowed DQN to learn policies directly from pixels and achieve human-level performance on various Atari 2600 games~\cite{mnih2015human}. Figure~\ref{fig:dqn_architecture} illustrates the core architecture of the DQN agent, while Algorithm \ref{alg:dqn} describes it's training process.

\begin{figure}[ht]
    \caption{Schematic of Deep Q-Network (DQN): images are processed through a CNN to extract features, which are then mapped to Q-values for each action. The target network stabilizes training by providing fixed TD targets.}
    \centering
    \includegraphics[width=0.75\textwidth]{images/fundamentacao/rl/dqn.jpeg}
    \label{fig:dqn_architecture}
    \par\medskip\ABNTEXfontereduzida\selectfont\textbf{Source:} Author
\end{figure}

\begin{algorithm}
\caption{Deep Q-Network}
\label{alg:dqn}
\begin{algorithmic}[1]
\Require Initialize Q-network with weights $\theta$
\Require Initialize target network with weights $\theta^- \leftarrow \theta$
\Require Initialize replay buffer $\mathcal{D}$
\For{each episode}
    \State Initialize $s$
    \For{each step $t$ in the episode}
        \State Select action $a_t$ using an $\epsilon$-greedy policy from $Q(s, a; \theta)$
        \State Execute $a_t$ and observe reward $r_{t+1}$ and next state $s_{t+1}$
        \State Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in $\mathcal{D}$
        \State Sample random mini-batch of transitions $(s_j, a_j, r_j, s_j')$ from $\mathcal{D}$
        \State Compute TD target:
        \[
        y_j = r_j + \gamma \max_{a'} Q(s_j', a'; \theta^-)
        \]
        \State Perform gradient descent on:
        \[
        \left(y_j - Q(s_j, a_j; \theta)\right)^2
        \]
        \State Every $C$ steps, update target network: $\theta^- \leftarrow \theta$
        \State $s \leftarrow s_{t+1}$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
