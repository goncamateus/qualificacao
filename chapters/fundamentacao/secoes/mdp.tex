\section{Finite Markov Decision Process}
\label{sec:mdp}

The \gls{mdp} is a mathematical framework used to formalize sequential decision-making problems in \gls{rl}, where the agent learns from incremental feedback. It provides a well-defined structure to model the interaction between an agent and its environment. As previously introduced, the agent observes the environment, takes actions, and receives feedback in the form of rewards. This interactive process, at its core, defines a \gls{mdp}~\cite{sutton2018reinforcement}.

\begin{figure}
    \caption{Representation of a \gls{mdp}. Source: Author}
    \centering
    \includegraphics[width=0.75\linewidth]{images/fundamentacao/rl/mdp.png}
    \label{fig:fundamentacao/mdp_basic}
    \par\medskip\ABNTEXfontereduzida\selectfont\textbf{Source:} Author
\end{figure}

The objective of the agent in an MDP is to maximize the expected \emph{return}, which quantifies the cumulative reward obtained over time. Formally, the return from time step $t$ is defined as
\begin{equation}
    \label{eq:return}
    G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k r_{t+k+1}
\end{equation}
where $r_{t+k+1}$ denotes the reward received at future time step $t+k+1$, and $\gamma \in [0,1]$ is the \emph{discount factor}. The discount factor controls the relative importance of future rewards with respect to immediate ones. Values of $\gamma$ close to zero bias the agent toward short-term rewards, whereas values close to one encourage long-term planning by assigning higher weight to delayed outcomes.

With this notion of return, a finite Markov Decision Process is formally defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $\mathcal{P}(s' \mid s,a)$ denotes the state transition probability from state $s$ to state $s'$ after taking action $a$, and $\mathcal{R}(s,a)$ represents the expected immediate reward associated with the stateâ€“action pair $(s,a)$.

At each discrete time step $t = 0, 1, 2, \dots$, the agent observes the current state $s_t \in \mathcal{S}$, selects an action $a_t \in \mathcal{A}(s_t)$, receives a scalar reward $r_{t+1} \in \mathbb{R}$, and transitions to a new state $s_{t+1} \in \mathcal{S}$ according to the transition probability $\mathcal{P}(s_{t+1} \mid s_t, a_t)$. This interaction loop, illustrated in Figure~\ref{fig:fundamentacao/mdp_basic}, continues until a terminal condition is reached or indefinitely in continuing tasks.



