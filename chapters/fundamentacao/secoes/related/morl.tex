\subsection{MORL Approaches}
\label{sec:morl_approaches}

A central theme in recent advances in Multi-Objective Reinforcement Learning is the idea of decomposition — the notion that a complex, multi-objective decision-making problem can be represented or solved through a structured combination of simpler subproblems. While most existing approaches employ this principle as a means to approximate the Pareto front or the \gls{ccs}, they differ in how they perform this decomposition: through preference conditioning, constrained subproblems, or policy aggregation. This section revisits the main algorithmic families that shaped modern \gls{morl} highlighting how each embodies this decomposition perspective, even if oriented toward Pareto-based evaluation.

\textbf{Envelope Q-Learning} \cite{yang-envelope} embodies decomposition at the level of the preference space. Instead of training independent agents for each scalarization of the objective vector, a single parametric Q-function is conditioned on a preference vector $\omega$, implicitly decomposing the learning process across the preference dimensions. The ``envelope'' operator defines a convex combination of value estimates that allows the agent to reuse experience gathered under one preference to update other regions of the space.

This shared representation transforms the multi-objective learning process into a structured interpolation problem over preferences, where the Q-function approximates a continuous mapping from (state, $\omega$) pairs to expected returns. Although its goal remains the coverage of the convex Pareto front, Envelope Q-Learning’s core mechanism demonstrates how linear preference decomposition can yield efficient and generalizable learning, providing the foundation for subsequent model-based and active-learning MORL methods.

\textbf{Generalized Policy Improvement Prioritization} (GPI Prioritization) \cite{alegre2023sample} extends the decomposition principle through policy reuse and prioritization. Rather than optimizing for all objectives simultaneously, the framework decomposes the multi-objective task into a collection of weighted single-objective subproblems, where each policy captures optimal behavior under a specific preference vector. Generalized Policy Improvement (GPI) then provides a mechanism to recombine these subpolicies, selecting or interpolating among them to improve performance across unseen preferences.

The GPI Linear Support (GPI-LS) and GPI-Prioritized Dyna (GPI-PD) variants further formalize this decomposition. GPI-LS adaptively identifies the most informative preferences to train on, while GPI-PD integrates model-based planning to prioritize simulated experiences that are most beneficial for policy improvement. These methods exemplify adaptive decomposition — dynamically selecting which regions of the objective space deserve attention — reinforcing the idea that multi-objective learning can be structured as a set of interacting single-objective problems guided by preference geometry.

Across these works, a unifying thread emerges: MORL algorithms inherently rely on decomposition — of objectives, preferences, or return spaces — as a mechanism for tractable learning. In both cases, the Pareto front serves as the algorithmic goal and less as the evaluation surface against which decomposed learning is assessed.

This decomposition view reframes MORL not as a search for the Pareto frontier, but as a structured method for disentangling multi-objective complexity into learnable components. These methods collectively demonstrate that efficient multi-objective behavior can emerge from decomposed structures—whether through conditioning, prioritization, or sequential subproblem solving—laying the conceptual foundation for approaches such as MORL/D, where decomposition itself becomes the primary object of study rather than an implicit design choice.

In contrast to the aforementioned approaches, this thesis investigates decomposition as an explicit learning principle, decoupled from Pareto-based objectives. The focus shifts from reconstructing the front to understanding how decomposed reward structures influence policy formation, sample efficiency, and adaptation. In doing so, MORL/D explores decomposition not as a means to an end, but as a fundamental property that can be directly optimized to yield interpretable, adaptable, and transferable multi-objective behavior.