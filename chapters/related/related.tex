\chapter{Related Work}
\label{chap:related}

This chapter situates \gls{dylam} within the relevant literature and clarifies its conceptual lineage. The review is organized into two complementary parts. First, we revisit the seminal contribution of \cite{russell2003q}, which fundamentally reframed reward specification in reinforcement learning by explicitly decoupling reward design from optimal behavior. Second, we survey representative approaches in \gls{morld} that directly informed our methodological choices. Finally, we examine how Curriculum Learning has been studied in the context of Reinforcement Learning and discuss recent works that are closely aligned with our proposed methodology.

Although the primary objective of this work is not to solve a multi-objective reinforcement learning problem, many of the concepts and mechanisms underlying \gls{dylam} originate from the \gls{morl} literature, such as reward decomposition, trade-offs between competing objectives, and adaptive weighting of reward signals. For this reason, the following review discusses \gls{morl} methods not as direct baselines, but as foundational work that motivates and contextualizes the design choices adopted in this thesis.


\input{chapters/related/decq}
\input{chapters/related/morl}
\input{chapters/related/curriculum}
