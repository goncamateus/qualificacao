\chapter{Related Work}
\label{chap:related}

This chapter situates \gls{dylam} within the relevant literature and clarifies its conceptual lineage. The review is organized into two complementary parts. First, we revisit the seminal contribution of \cite{russell2003q}, which fundamentally reframed the problem of reward specification in reinforcement learning by explicitly separating the notions of reward and optimal behavior. Second, we survey representative approaches from \gls{morld} that directly informed our methodological design.

Although the primary objective of this work is not to solve a multi-objective reinforcement learning problem, many of the concepts and mechanisms underlying \gls{dylam} originate from the \gls{morl} literature, such as reward decomposition, trade-offs between competing objectives, and adaptive weighting of reward signals. For this reason, the following review discusses \gls{morl} methods not as direct baselines, but as foundational work that motivates and contextualizes the design choices adopted in this thesis.


\input{chapters/related/decq}
\input{chapters/related/morl}
\input{chapters/related/curriculum}
