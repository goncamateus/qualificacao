\subsection{Curriculum Learning on Reinforcement Learning context}
\label{sec:curriculum}

\gls{cl} is a training paradigm inspired by human education and animal training, often referred to as shaping, in which learning experiences are organized in a meaningful sequence that leverages previously acquired knowledge~\cite{curriculum}. The fundamental principle is to expose the learner to simpler or more structured tasks early in training and gradually increase task difficulty. This approach has been shown to accelerate convergence, improve learning stability, and, in many cases, enhance final performance. In \gls{rl}, curricula define structured learning trajectories that promote efficient knowledge transfer from intermediate source tasks to a challenging target task, rather than requiring the agent to learn the target \gls{mdp} directly from scratch~\cite{narvekar2020curriculum}.

\subsubsection{Types of Curriculum Learning}
The curriculum learning literature commonly categorizes methods according to the degree of control exercised over the environment and which components of the \gls{mdp} are modified during training~\cite{narvekar2020curriculum}.

\textbf{Sample Sequencing.} At one extreme are sample sequencing approaches, which do not alter the environment at all but instead reorder or reweight experience collected from the target task. A canonical example is Prioritized Experience Replay~\cite{hessel2017rainbowcombiningimprovementsdeep}, which biases sampling toward transitions with high temporal-difference errors in order to focus learning on more informative experiences. Closely related target-based curricula similarly operate solely on the final task, inducing an implicit curriculum through experience selection rather than explicit task construction.

\textbf{Co-Learning.} The task difficulty emerges implicitly from interactions between multiple agents~\cite{sukhbaatar2017intrinsic, pinto2017robust, wang2020few,yang2018cm3}. These approaches are prevalent in multi-agent reinforcement learning and include self-play paradigms, in which agents face increasingly competent versions of themselves. As learning progresses, the curriculum is shaped naturally by the evolving competence of opponents or collaborators, without requiring an explicit notion of task difficulty.

\textbf{Reward and Initial/Terminal State Distribution Changes.} A more constrained but expressive class of curriculum methods modifies the reward function or the initial and terminal state distributions while keeping the state space, action space, and transition dynamics fixed~\cite{asada1996purposive, baranes2013active}. Reverse curriculum generation exemplifies this approach by initializing the agent near a goal state during early training and progressively increasing the distance to the goal as performance improves. Such methods preserve the core structure of the environment while controlling learning difficulty through carefully chosen task parameterizations.

\textbf{No-Restrictions Curricula.} In this setting, intermediate tasks may vary arbitrarily in their transition dynamics, reward functions, or action spaces, and curricula are often formalized as Curriculum MDPs, where a teacher mechanism selects tasks to optimize the learning efficiency of a student agent~\cite{silva2018object, bassich2020curriculum, li2023understanding}. While this generality enables highly flexible curricula, it typically necessitates explicit mechanisms for transferring knowledge across tasks and managing distributional shifts.

\subsubsection{No-Restrictions Curricula Approaches}

Beyond the question of which components of the \gls{mdp} are modified, curriculum learning methods also differ in how task progression is defined and controlled throughout training. This perspective emphasizes the process by which curricula are specified, ranging from fully predefined schedules to mechanisms that adapt online in response to the agent’s learning dynamics. From this viewpoint, curricula may be designed explicitly by domain experts, derived directly from the target task, or generated automatically through adaptive progression rules.

Static curricula are specified prior to training and rely on detailed domain knowledge about the agent and the environment. Such approaches have been successfully applied in robotics, for instance through overlapping layered learning in simulated robot soccer, where complex behaviors are acquired through carefully staged subtasks~\cite{asada1996purposive, riedmiller2018learning}. In contrast, automatic curriculum generation methods construct tasks online, using predefined rules or performance-driven adaptation mechanisms, including generative models that discover and expand goal regions to guide exploration~\cite{silva2018object, narvekar2017autonomous}. Between these extremes lie target-based curricula, which operate exclusively on the final task by reordering experience or goals without introducing explicit intermediate tasks~\cite{andrychowicz2017hindsight, fang2019curriculum}.

\textbf{Progression Functions}~\cite{bassich2020curriculum} introduce a principled framework for online curriculum generation that decouples the estimation of task difficulty from the construction of tasks themselves. Their approach relies on a progression function, which computes a scalar complexity value in the range $[0,1]$ reflecting the agent’s current capabilities, and a mapping function, which translates this value into a concrete \gls{mdp} by encoding domain knowledge about how task parameters influence difficulty. Task complexity may evolve according to predefined schedules or adapt dynamically based on performance, enabling a continuously varying curriculum that addresses the question of how long an agent should train on each intermediate task and empirically outperforms static sequencing strategies.

\textbf{ROLLIN}~\cite{li2023understanding} examine curriculum learning from a theoretical standpoint, focusing on when reformulating a single-task reinforcement learning problem as a curriculum of related tasks yields sample efficiency gains. They show that sparse-reward problems can induce exponential sample complexity for standard stochastic policy gradient methods under poor initialization. To mitigate this effect, the \textsc{ROLLIN} algorithm constructs a curriculum in which consecutive tasks are close in terms of their optimal state visitation distributions. By initializing each task’s policy from the previous optimum and defining an initial state distribution that interpolates between visitation distributions, they prove that, under Lipschitz continuity assumptions on the reward function, curriculum learning reduces sample complexity from exponential to polynomial.

Collectively, these approaches illustrate that curriculum learning is not only characterized by the structure of intermediate tasks, but also by the mechanisms through which learning emphasis shifts over time. This distinction provides a useful lens for interpreting methods that induce curriculum effects implicitly, without explicit task generation, and motivates the analysis of optimization-driven curricula such as the one explored in this work.
