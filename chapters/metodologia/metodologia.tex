\chapter{Methodology}
\label{chap:method}

In this chapter, we formalize the methods proposed in this work. Drawing inspiration from the techniques discussed in Chapter~\ref{chap:background}, we aim to develop a minimally intrusive framework capable of adapting to the training context and enhancing the reward component that is most relevant at each moment.

We begin by introducing a high-level overview of the proposed methodology. Then, we detail the mathematical formulation of both the static and dynamic reward weighting strategies used in our experiments. Finally, we present an analysis on a toy problem designed to show the outcomes of our method.

\input{chapters/metodologia/secoes/sufficient_pareto}
\input{chapters/metodologia/secoes/reward_weighting_methods}
\input{chapters/metodologia/secoes/reward_weighting_methods/fixlam}
\input{chapters/metodologia/secoes/reward_weighting_methods/dylam}
\input{chapters/metodologia/secoes/chicken_banana}
