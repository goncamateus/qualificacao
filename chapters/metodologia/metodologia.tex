\chapter{Methodology}
\label{chap:method}

In this chapter, we formalize the methods proposed in this work. Drawing inspiration from the techniques discussed in Chapter~\ref{chap:background}, we aim to develop a minimally intrusive framework capable of adapting to the training context and enhancing the reward component that is most relevant at each moment.

We begin by introducing the notion of sufficient values in the context of \gls{morl}. We then provide a high-level overview of the proposed methodology, followed by a detailed mathematical formulation of the static and dynamic reward-weighting strategies employed in our experiments. Finally, we analyze a toy problem designed to illustrate the qualitative behavior and outcomes of the proposed approach.

\input{chapters/metodologia/secoes/sufficient_pareto}
\input{chapters/metodologia/secoes/reward_weighting_methods}
\input{chapters/metodologia/secoes/reward_weighting_methods/fixlam}
\input{chapters/metodologia/secoes/reward_weighting_methods/dylam}
\input{chapters/metodologia/secoes/chicken_banana}
