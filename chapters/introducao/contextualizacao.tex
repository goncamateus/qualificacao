\section{Contextualization}
\label{sec:contextualization}

% Historical context
%   - AI Boom, Growth of LLMs, Foundation models
% Context for Machine Learning
%   - Back to the begin: ML basics, 
% Context of Reinforcement Learning
%   - Intro to what is Reinforcement Learning focusing on how an agent learns, What is an exeperience, The impact of deep learning on deep q-networks
% Applications of Reinforcement Learning
%   - Robotics, Games, Large Language Models

The field of Artificial Intelligence (AI) has undergone a remarkable evolution over the past few decades, fueled by exponential growth in computational resources, the proliferation of large-scale datasets, and groundbreaking algorithmic innovations~\cite{aiboom1, aiboom2}.
This period, often referred to as the \textit{AI Boom}, has been defined by the emergence of \textbf{Large Language Models (LLMs)} and \textbf{Foundation Models}—versatile architectures trained on vast and diverse datasets, capable of performing a wide array of tasks with minimal task-specific adaptation~\cite{minaee2025largelanguagemodelssurvey}.
These models have not only pushed the boundaries of what machines can achieve in domains like natural language understanding, code generation, and problem-solving but have also reshaped the broader landscape of AI research, emphasizing scalability, adaptability, and generalizability~\cite{geminiteam2025geminifamilyhighlycapable, deepseekai2025deepseekr1incentivizingreasoningcapability, openai2024gpt4technicalreport}.

% FIGURINHA ILUSTRANDO A CRESCENTE DA IA NA VIDA DAS PESSOAS

At the heart of these advancements lies \textbf{Machine Learning (ML)}, the subfield of AI that enables systems to learn from data rather than relying on explicit programming. ML encompasses a spectrum of approaches, from \textbf{supervised learning}, where models are trained on labeled datasets to predict outcomes, to \textbf{unsupervised learning}, which uncovers hidden patterns in unannotated data~\cite{russell2016artificial}.
Yet, among these paradigms, \textbf{Reinforcement Learning (\gls{rl})} stands out as a framework where learning is driven by interaction. In \gls{rl}, an agent learns to make decisions by engaging with its environment: it observes the current state $s$, selects an action $a$, and receives a reward $r$ that reflects the quality of its choice. Over time, the agent refines its strategy—formalized as a \textbf{policy} $\pi(a|s)$—to maximize cumulative reward, often guided by principles like the Bellman equation \cite{sutton2018reinforcement}.

The versatility of \gls{rl} has led to its adoption across a diverse range of applications without labels of ``better actions'', but are rewarded for taking such decisions. In \textbf{robotics}, \gls{rl} enables machines to learn intricate motor skills, from dexterous manipulation to dynamic locomotion, adapting to real-world variability in ways traditional control systems cannot~\cite{intelligence2025pi, todorov2012mujoco, rsoccer}. In \textbf{games}, \gls{rl} agents have surpassed human experts in strategically complex environments like Go, Chess, Dota2, and StarCraft II, showcasing their ability to master long-term planning and decision-making under uncertainty~\cite{silver2016mastering, silver2018general, berner2019dota, vinyals2019grandmaster}. More recently, \gls{rl} has played a pivotal role in refining LLMs through techniques like \textbf{Reinforcement Learning from Human Feedback (RLHF)}, which aligns model outputs with human preferences to produce more coherent, 
contextually appropriate, and safer responses~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.
These successes, however, come with significant challenges, particularly in the design and implementation of \gls{rl} environments.

One of the most persistent hurdles in \gls{rl} is environment design, where the formulation of rewards, state representations, and transition dynamics can decisively determine learning success or failure \cite{towers2024gymnasium}. Among these components, reward function design plays a particularly critical role when multiple, potentially conflicting objectives must be simultaneously satisfied, a setting that naturally arises in many real-world control and decision-making problems. In such scenarios, the agent must balance trade-offs—such as efficiency versus safety in mobile robot navigation or task completion versus energy consumption—making the reward signal inherently multi-objective rather than scalar by nature. Naïvely aggregating these objectives into a single reward can bias learning toward dominant components, obscure secondary goals, or induce unstable learning dynamics.

Poorly specified reward functions may also give rise to unintended behaviors, including reward hacking, in which the agent exploits loopholes in the reward definition to maximize return without accomplishing the intended task \cite{sutton2018reinforcement}. These issues are further exacerbated in sparse-reward environments, where informative feedback is infrequent and learning progress is slow. In this context, the exploration–exploitation trade-off becomes particularly acute, as agents must discover meaningful behaviors with little guidance from the environment.

A common mitigation strategy is reward shaping, which augments the original reward signal with additional feedback designed to guide exploration and accelerate learning \cite{ng1999policy}. While effective in practice, reward shaping introduces its own challenges: improperly designed shaping terms can alter the optimal policy, inject human bias, or over-constrain exploration, leading to suboptimal or brittle solutions. Even potential-based shaping methods, which preserve policy optimality under certain conditions, require careful design and do not address how multiple reward components should be prioritized or adapted throughout training. These limitations highlight the need for more principled and systematic approaches to reward decomposition, prioritization, and adaptation—motivating frameworks that explicitly reason about multiple reward signals and their dynamic influence on learning.
