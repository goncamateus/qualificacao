\section{Hypothesis}
\label{sec:hypothesis}

Motivated by this challenge, drawing inspiration from recent advances in automated curriculum learning \cite{portelas2020automatic}, we explore how the priorities assigned to different reward components can be adaptively modified during training. Rather than relying on a fixed reward composition, we analyze the impact of each component on the cumulative reward identifying \textit{sufficient values} beyond which the component no longer significantly contributes to task success. This analysis allows us to refine the reward weighting dynamically over time, ensuring that under-optimized components are emphasized until their contribution reaches a desirable threshold.

Our approach also extends ideas from \gls{morl}, particularly those involving the decomposition of the reward function into independently learned value functions \cite{felten2024multi}. While many \gls{morl} techniques aim to generate optimal policies along the Pareto front \cite{pgmorl} or learn adaptable policies conditioned on preference vectors \cite{capql}, our focus is distinct: we maintain a single policy and \textbf{optimize all reward components in parallel}, prioritizing those that have not yet met their sufficient value thresholds. \textbf{\textit{The ultimate goal is to maximize the cumulative reward}} by intelligently redistributing focus throughout the agent's development.

To this end, we build upon the concept of the \textit{Dynamic Lambda Weights for Reward Signals} (DyLam) \cite{machado2025dylam}, a framework that dynamically adjusts reward weights during training based on the progress of individual components. DyLam learns a separate value function for each component and updates the reward weights $\lambda$ accordingly, encouraging the agent to focus on lagging objectives. This adaptive process results in a more balanced and efficient learning trajectory, capable of bridging the gap between single-objective performance and multi-objective awareness. In this context, DyLam serves as a foundation for investigating how dynamic curricula can enhance learning efficiency, reduce the need for manual reward tuning, and promote robust policy acquisition in environments with complex, structured tasks.
