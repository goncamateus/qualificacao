\section{Objectives}
\label{sec:objectives}

The primary objective of this work is to \textbf{investigate and extend the concept of dynamic reward weighting} in Reinforcement Learning environments characterized by multiple reward components. Specifically, this dissertation aims to:

\begin{itemize}
\item Conduct a comprehensive literature review of Reinforcement Learning frameworks focused on learning multi/sub-objectives;
\item Extend the \gls{dylam} framework to discrete state and action spaces;
\item Demonstrate that \gls{dylam} effectively explores the reward-weight space, leading to balanced policy learning;
\item Enable adaptive and efficient optimization of the cumulative reward by dynamically prioritizing under-optimized objectives;
\end{itemize}

To support these goals, the remainder of this dissertation is structured as follows:
\textbf{Theoretical Background} introduces the foundational concepts of Reinforcement Learning, Multi-Objective Reinforcement Learning, and the relevant frameworks for building training environments.  
\textbf{Related Works} presents and discusses prior research that aligns with or informs the goals of this dissertation.  
\textbf{Methodology} details the proposed approach, including the extension of DyLam to discrete domains, and the theoretical and empirical justification for its use.  
\textbf{Results} presents experimental outcomes, compares the performance of DyLam with related methods, and analyzes its behavior in various learning scenarios.  
\textbf{Conclusion} summarizes the main findings, evaluates the achievement of the stated objectives, and outlines future research directions for further improving dynamic reward weighting strategies in RL.

