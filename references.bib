@misc{aiboom1,
    title = {The AI boom is happening all over the world, and it’s accelerating quickly},
    author= {Statt, Nick},
    howpublished = {https://www.theverge.com/2018/12/12/18136929/artificial-intelligence-ai-index-report-2018-machine-learning-global-progress-research}, 
    year = 2018,
    note = {Accessed: 2025-05-17},
}

@misc{aiboom2,
    title = {Google’s Gemini Is the Real Start of the Generative AI Boom},
    author= {Knight, Will},
    howpublished = {https://www.wired.com/story/google-gemini-generative-ai-boom/}, 
    year = 2023,
    note = {Accessed: 2025-05-17},
}

@misc{minaee2025largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2025},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

@misc{geminiteam2025geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team},
      year={2025},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2016},
  publisher={pearson}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  edition={2nd},
  publisher={MIT press},
  address={Cambridge, MA}
}

@article{intelligence2025pi,
  title={$\pi0.5$: a Vision-Language-Action Model with Open-World Generalization},
  author={Intelligence, Physical and Black, Kevin and Brown, Noah and Darpinian, James and Dhabalia, Karan and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and others},
  journal={arXiv preprint arXiv:2504.16054},
  year={2025}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@InProceedings{rsoccer,
author="Martins, Felipe B.
and Machado, Mateus G.
and Bassani, Hansenclever F.
and Braga, Pedro H. M.
and Barros, Edna S.",
editor="Alami, Rachid
and Biswas, Joydeep
and Cakmak, Maya
and Obst, Oliver",
title="rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer",
booktitle="RoboCup 2021: Robot World Cup XXIV",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="165--176",
abstract="Reinforcement learning is an active research area with a vast number of applications in robotics, and the RoboCup competition is an interesting environment for studying and evaluating reinforcement learning methods. A known difficulty in applying reinforcement learning to robotics is the high number of experience samples required, being the use of simulated environments for training the agents followed by transfer learning to real-world (sim-to-real) a viable path. This article introduces an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments. We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two state-of-the-art reinforcement learning methods as well as their limitations in certain scenarios introduced in this framework. We believe this will make it easier for more teams to compete in these categories using end-to-end reinforcement learning approaches and further develop this research area.",
isbn="978-3-030-98682-7"
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999},
  organization={Citeseer}
}

@article{towers2024gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

@misc{curriculum,
  doi = {10.48550/ARXIV.2010.13166},
  
  url = {https://arxiv.org/abs/2010.13166},
  
  author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Survey on Curriculum Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{portelas2020automatic,
  title={Automatic curriculum learning for deep rl: A short survey},
  author={Portelas, R{\'e}my and Colas, C{\'e}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:2003.04664},
  year={2020}
}

@article{felten2024multi,
  title={Multi-objective reinforcement learning based on decomposition: A taxonomy and framework},
  author={Felten, Florian and Talbi, El-Ghazali and Danoy, Gr{\'e}goire},
  journal={Journal of Artificial Intelligence Research},
  volume={79},
  pages={679--723},
  year={2024}
}

@inproceedings{capql,
  title={Multi-objective reinforcement learning: Convexity, stationarity and pareto optimality},
  author={Lu, Haoye and Herman, Daniel and Yu, Yaoliang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{pgmorl,
  title={Prediction-guided multi-objective reinforcement learning for continuous robot control},
  author={Xu, Jie and Tian, Yunsheng and Ma, Pingchuan and Rus, Daniela and Sueda, Shinjiro and Matusik, Wojciech},
  booktitle={International conference on machine learning},
  pages={10607--10616},
  year={2020},
  organization={PMLR}
}

@article{gpi,
  title={Generalized policy improvement algorithms with theoretically supported sample reuse},
  author={Queeney, James and Paschalidis, Ioannis Ch and Cassandras, Christos G},
  journal={IEEE Transactions on Automatic Control},
  year={2024},
  publisher={IEEE}
}

@article{emmerich2018tutorial,
  title={A tutorial on multiobjective optimization: fundamentals and evolutionary methods},
  author={Emmerich, Michael TM and Deutz, Andr{\'e} H},
  journal={Natural computing},
  volume={17},
  pages={585--609},
  year={2018},
  publisher={Springer}
}

@article{ehrgott2012vilfredo,
  title={Vilfredo Pareto and multi-objective optimization},
  author={Ehrgott, Matthias},
  journal={Doc. math},
  volume={8},
  pages={447--453},
  year={2012}
}

@article{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher JCH},
  year={1989},
  institution={King's College, Cambridge}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015}
}

@article{tesauro1994td,
  title={TD-Gammon, a self-teaching backgammon program, achieves master-level play},
  author={Tesauro, Gerald},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={215--219},
  year={1994}
}

@article{dqnTarget,
  title={Double Q-learning},
  author={Hasselt, Hado Van and Guez, Arthur and Silver, David},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={2094--2102},
  year={2015}
}

@article{goodfellow2013multi,
  title={Multi-digit number recognition from street view imagery using deep convolutional neural networks},
  author={Goodfellow, Ian and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  journal={arXiv preprint arXiv:1312.6082},
  year={2013}
}

@misc{ddpg,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{dpg,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

@inproceedings{offpac,
  title={Model-free reinforcement learning with continuous action in practice},
  author={Degris, Thomas and Pilarski, Patrick M and Sutton, Richard S},
  booktitle={2012 American Control Conference (ACC)},
  pages={2177--2182},
  year={2012},
  organization={IEEE}
}

@article{nfqca,
  title={Reinforcement learning in feedback control},
  author={Hafner, Roland and Riedmiller, Martin},
  journal={Machine learning},
  volume={84},
  number={1-2},
  pages={137--169},
  year={2011},
  publisher={Springer}
}

@article{uhlenbeck1930theory,
  title={On the theory of the Brownian motion},
  author={Uhlenbeck, George E and Ornstein, Leonard S},
  journal={Physical review},
  volume={36},
  number={5},
  pages={823},
  year={1930},
  publisher={APS}
}

@article{sac,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Kristian Hartikainen and
                  George Tucker and
                  Sehoon Ha and
                  Jie Tan and
                  Vikash Kumar and
                  Henry Zhu and
                  Abhishek Gupta and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic Algorithms and Applications},
  journal      = {CoRR},
  volume       = {abs/1812.05905},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.05905},
  eprinttype    = {arXiv},
  eprint       = {1812.05905},
  timestamp    = {Wed, 14 May 2025 08:12:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-05905.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain},
  author={Rosenblatt, Frank},
  journal={Psychological Review},
  volume={65},
  number={6},
  pages={386--408},
  year={1958},
  publisher={American Psychological Association}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Pergamon}
}

@article{dubey2021activation,
  title={Activation functions in deep learning: A comprehensive survey and benchmark},
  author={Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  journal={Neurocomputing},
  volume={467},
  pages={268--295},
  year={2022},
  publisher={Elsevier}
}

@article{lederer2021activation,
  title={Activation Functions in Artificial Neural Networks: A Systematic Overview},
  author={Lederer, Johannes},
  journal={arXiv preprint arXiv:2101.09957},
  year={2021}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}


@book{kroese2014monte,
  title={Monte Carlo Methods in Financial Engineering},
  author={Kroese, Dirk P. and Brereton, Tim and Taimre, Thomas and Botev, Zdravko I.},
  year={2014},
  publisher={Springer},
  address={New York}
}

@article{hayes2022practical,
  title={A practical guide to multi-objective reinforcement learning and planning},
  author={Hayes, Conor F and R{\u{a}}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M and Dazeley, Richard and Heintz, Fredrik and others},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={1},
  pages={26},
  year={2022},
  publisher={Springer}
}

@article{silver2021reward,
  title={Reward is enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S},
  journal={Artificial intelligence},
  volume={299},
  pages={103535},
  year={2021},
  publisher={Elsevier}
}

@article{roijers2013survey,
  title={A survey of multi-objective sequential decision-making},
  author={Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
  journal={Journal of Artificial Intelligence Research},
  volume={48},
  pages={67--113},
  year={2013}
}

@misc{mossalam2016multiobjectivedeepreinforcementlearning,
      title={Multi-Objective Deep Reinforcement Learning}, 
      author={Hossam Mossalam and Yannis M. Assael and Diederik M. Roijers and Shimon Whiteson},
      year={2016},
      eprint={1610.02707},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1610.02707}, 
}

@article{chen2020combining,
author = {Chen, Diqi and Wang, Yizhou and Gao, Wen},
title = {Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01702-7},
doi = {10.1007/s10489-020-01702-7},
abstract = {Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method.},
journal = {Applied Intelligence},
month = oct,
pages = {3301–3317},
numpages = {17},
keywords = {Sampling efficiency, Pareto frontier, Multi-policy reinforcement learning, Multi-objective reinforcement learning}
}

@misc{abels2019dynamicweightsmultiobjectivedeep,
      title={Dynamic Weights in Multi-Objective Deep Reinforcement Learning}, 
      author={Axel Abels and Diederik M. Roijers and Tom Lenaerts and Ann Nowé and Denis Steckelmacher},
      year={2019},
      eprint={1809.07803},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.07803}, 
}

@article{castelletti2013multi,
author = {Castelletti, A. and Pianosi, F. and Restelli, M.},
title = {A multiobjective reinforcement learning approach to water resources systems operation: Pareto frontier approximation in a single run},
journal = {Water Resources Research},
volume = {49},
number = {6},
pages = {3476-3486},
keywords = {multiobjective optimization, reservoir operation, reinforcement learning},
doi = {https://doi.org/10.1002/wrcr.20295},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/wrcr.20295},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/wrcr.20295},
abstract = {The operation of large-scale water resources systems often involves several conflicting and noncommensurable objectives. The full characterization of tradeoffs among them is a necessary step to inform and support decisions in the absence of a unique optimal solution. In this context, the common approach is to consider many single objective problems, resulting from different combinations of the original problem objectives, each one solved using standard optimization methods based on mathematical programming. This scalarization process is computationally very demanding as it requires one optimization run for each trade-off and often results in very sparse and poorly informative representations of the Pareto frontier. More recently, bio-inspired methods have been applied to compute an approximation of the Pareto frontier in one single run. These methods allow to acceptably cover the full extent of the Pareto frontier with a reasonable computational effort. Yet, the quality of the policy obtained might be strongly dependent on the algorithm tuning and preconditioning. In this paper we propose a novel multiobjective Reinforcement Learning algorithm that combines the advantages of the above two approaches and alleviates some of their drawbacks. The proposed algorithm is an extension of fitted Q-iteration (FQI) that enables to learn the operating policies for all the linear combinations of preferences (weights) assigned to the objectives in a single training process. The key idea of multiobjective FQI (MOFQI) is to enlarge the continuous approximation of the value function, that is performed by single objective FQI over the state-decision space, also to the weight space. The approach is demonstrated on a real-world case study concerning the optimal operation of the HoaBinh reservoir on the Da river, Vietnam. MOFQI is compared with the reiterated use of FQI and a multiobjective parameterization-simulation-optimization (MOPSO) approach. Results show that MOFQI provides a continuous approximation of the Pareto front with comparable accuracy as the reiterated use of FQI. MOFQI outperforms MOPSO when no a priori knowledge on the operating policy shape is available, while produces slightly less accurate solutions when MOPSO can exploit such knowledge.},
year = {2013}
}

@article{alegre2023sample,
  title={Sample-efficient multi-objective learning via generalized policy improvement prioritization},
  author={Alegre, Lucas N and Bazzan, Ana LC and Roijers, Diederik M and Now{\'e}, Ann and da Silva, Bruno C},
  journal={arXiv preprint arXiv:2301.07784},
  year={2023}
}

@inproceedings{roijers2018multi,
  title={Multi-objective reinforcement learning for the expected utility of the return},
  author={Roijers, Diederik M and Steckelmacher, Denis and Now{\'e}, Ann},
  booktitle={Proceedings of the Adaptive and Learning Agents workshop at FAIM},
  volume={2018},
  year={2018}
}

@inproceedings{MinigridMiniworld23,
  author       = {Maxime Chevalier{-}Boisvert and Bolun Dai and Mark Towers and Rodrigo Perez{-}Vicente and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  title        = {Minigrid {\&} Miniworld: Modular {\&} Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  booktitle    = {Advances in Neural Information Processing Systems 36, New Orleans, LA, USA},
  month        = {December},
  year         = {2023},
}

@Article{bellemare13arcade,
    author = {{Bellemare}, M.~G. and {Naddaf}, Y. and {Veness}, J. and {Bowling}, M.},
    title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
    journal = {Journal of Artificial Intelligence Research},
    year = "2013",
    month = "jun",
    volume = "47",
    pages = "253--279",
}

@inproceedings{felten_toolkit_2023,
	author = {Felten, Florian and Alegre, Lucas N. and Now{\'e}, Ann and Bazzan, Ana L. C. and Talbi, El Ghazali and Danoy, Gr{\'e}goire and Silva, Bruno C. {\relax da}},
	title = {A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning},
	booktitle = {Proceedings of the 37th Conference on Neural Information Processing Systems ({NeurIPS} 2023)},
	year = {2023}
}

@inproceedings{machado2025dylam,
  title={DyLam: A Dynamic Reward Weighting Framework for Reinforcement Learning Algorithms},
  author={Machado, Mateus and Bassani, Hansenclever},
  booktitle={Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems},
  pages={2651--2653},
  year={2025}
}

@inproceedings{russell2003q,
  title={Q-decomposition for reinforcement learning agents},
  author={Russell, Stuart J and Zimdars, Andrew},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={656--663},
  year={2003}
}

@inproceedings{brys2014multi,
  title={Multi-objectivization of reinforcement learning problems by reward shaping},
  author={Brys, Tim and Harutyunyan, Anna and Vrancx, Peter and Taylor, Matthew E and Kudenko, Daniel and Now{\'e}, Ann},
  booktitle={2014 international joint conference on neural networks (IJCNN)},
  pages={2315--2322},
  year={2014},
  organization={IEEE},
}

@article{huang2022cleanrl,
  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and João G.M. Araújo},
  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {274},
  pages   = {1--18},
  url     = {http://jmlr.org/papers/v23/21-1342.html}
}

@inproceedings{ropke-ipro,
author = {R\"{o}pke, Willem and Reymond, Mathieu and Mannion, Patrick and Roijers, Diederik M and Now\'{e}, Ann and R\u{a}dulescu, Roxana},
title = {Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {An important challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies to attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), which decomposes finding the Pareto front into a sequence of constrained single-objective problems. This enables us to guarantee convergence while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. We evaluate IPRO using utility-based metrics and its hypervolume and find that it matches or outperforms methods that require additional assumptions. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as planning and pathfinding.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1774–1783},
numpages = {10},
keywords = {multi-objective, pareto front, reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inbook{yang-envelope,
author = {Yang, Runzhe and Sun, Xingyuan and Narasimhan, Karthik},
title = {A generalized algorithm for multi-objective reinforcement learning and policy adaptation},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a new algorithm for multi-objective reinforcement learning (MORL) with linear preferences, with the goal of enabling few-shot adaptation to new tasks. In MORL, the aim is to learn policies over multiple competing objectives whose relative importance (preferences) is unknown to the agent. While this alleviates dependence on scalar reward design, the expected return of a policy can change significantly with varying preferences, making it challenging to learn a single model to produce optimal policies under different preference conditions. We propose a generalized version of the Bellman equation to learn a single parametric representation for optimal policies over the space of all possible preferences. After an initial learning phase, our agent can execute the optimal policy under any given preference, or automatically infer an underlying preference with very few samples. Experiments across four different domains demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1311},
numpages = {12}
}

@inproceedings{reymond-pcn,
author = {Reymond, Mathieu and Bargiacchi, Eugenio and Now\'{e}, Ann},
title = {Pareto Conditioned Networks},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In multi-objective optimization, learning all the policies that reach Pareto-efficient solutions is an expensive process. The set of optimal policies can grow exponentially with the number of objectives, and recovering all solutions requires an exhaustive exploration of the entire state space. We propose Pareto Conditioned Networks (PCN), a method that uses a single neural network to encompass all non-dominated policies. PCN associates every past transition with its episode's return. It trains the network such that, when conditioned on this same return, it should reenact said transition. In doing so we transform the optimization problem into a classification problem. We recover a concrete policy by conditioning the network on the desired Pareto-efficient solution. Our method is stable as it learns in a supervised fashion, thus avoiding moving target issues. Moreover, by using a single network, PCN scales efficiently with the number of objectives. Finally, it makes minimal assumptions on the shape of the Pareto front, which makes it suitable to a wider range of problems than previous state-of-the-art multi-objective reinforcement learning algorithms.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1110–1118},
numpages = {9},
keywords = {multi policy, multi objective reinforcement learning, Pareto front},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}


@misc{hessel2017rainbowcombiningimprovementsdeep,
      title={Rainbow: Combining Improvements in Deep Reinforcement Learning}, 
      author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
      year={2017},
      eprint={1710.02298},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1710.02298}, 
}

@inproceedings{juozapaitis2019explainable,
  title={Explainable reinforcement learning via reward decomposition},
  author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale},
  booktitle={IJCAI/ECAI Workshop on explainable artificial intelligence},
  year={2019}
}